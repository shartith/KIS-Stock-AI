"""
Train Local Model - ë¡œì»¬ LLM ë¯¸ì„¸ì¡°ì • (Fine-tuning)
Unslothë¥¼ ì‚¬ìš©í•˜ì—¬ Qwen/Llama ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³ , GGUFë¡œ ë³€í™˜í•˜ì—¬ Ollamaì— ë“±ë¡í•©ë‹ˆë‹¤.
"""
import os
import torch
import subprocess
from dataset_builder import DatasetBuilder
from transformers import TrainingArguments
from trl import SFTTrainer

# Unsloth ë¼ì´ë¸ŒëŸ¬ë¦¬ (í•„ìˆ˜)
try:
    from unsloth import FastLanguageModel
    HAS_UNSLOTH = True
except ImportError:
    HAS_UNSLOTH = False
    print("âš ï¸ Unsloth not found. Please install it for efficient training.")

def run_command(cmd):
    """ì‰˜ ëª…ë ¹ì–´ ì‹¤í–‰"""
    print(f"Executing: {cmd}")
    process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    out, err = process.communicate()
    if process.returncode != 0:
        print(f"Error: {err.decode('utf-8')}")
        raise Exception(f"Command failed: {cmd}")
    return out.decode('utf-8')

def train_and_register_ollama(base_model_name = "unsloth/Qwen2.5-7B-Instruct-bnb-4bit", 
                              new_model_name = "qwen-stock-trader"):
    print(f"ğŸš€ Starting training pipeline for {base_model_name}...")
    
    if not HAS_UNSLOTH:
        raise ImportError("Unsloth library is required for this pipeline. (pip install unsloth)")

    # 1. ë°ì´í„°ì…‹ ì¤€ë¹„
    builder = DatasetBuilder()
    data_files = builder.get_all_data_files()
    
    from datasets import load_dataset
    # ì—¬ëŸ¬ íŒŒì¼ì„ í•˜ë‚˜ì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¡œë“œ
    dataset = load_dataset("json", data_files=data_files, split="train")
    print(f"ğŸ“š Loaded {len(dataset)} training examples from {len(data_files)} files")

    # 2. ëª¨ë¸ ë¡œë“œ (Qwen 2.5)
    max_seq_length = 2048
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = base_model_name,
        max_seq_length = max_seq_length,
        dtype = None,
        load_in_4bit = True,
    )
    
    # LoRA ì–´ëŒ‘í„° ì¶”ê°€
    model = FastLanguageModel.get_peft_model(
        model,
        r = 16,
        target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                          "gate_proj", "up_proj", "down_proj",],
        lora_alpha = 16,
        lora_dropout = 0,
        bias = "none",
        use_gradient_checkpointing = True,
    )

    # 3. í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ… (Qwen ChatML ìŠ¤íƒ€ì¼)
    # Qwenì€ ChatML í¬ë§·ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì´ì— ë§ì¶°ì•¼ í•¨
    def formatting_prompts_func(examples):
        instructions = examples["instruction"]
        outputs = examples["output"]
        texts = []
        for instruction, output in zip(instructions, outputs):
            # ChatML Format
            text = f"<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n{output}<|im_end|>"
            texts.append(text)
        return {"text": texts}

    # 4. í•™ìŠµ ì„¤ì •
    trainer = SFTTrainer(
        model = model,
        tokenizer = tokenizer,
        train_dataset = dataset,
        dataset_text_field = "text",
        max_seq_length = max_seq_length,
        dataset_num_proc = 2,
        formatting_func = formatting_prompts_func,
        args = TrainingArguments(
            per_device_train_batch_size = 2,
            gradient_accumulation_steps = 4,
            warmup_steps = 5,
            max_steps = 60, # ë°ì´í„° ì–‘ì— ë”°ë¼ ì¡°ì • í•„ìš”
            learning_rate = 2e-4,
            fp16 = not torch.cuda.is_bf16_supported(),
            bf16 = torch.cuda.is_bf16_supported(),
            logging_steps = 1,
            optim = "adamw_8bit",
            weight_decay = 0.01,
            lr_scheduler_type = "linear",
            seed = 3407,
            output_dir = "outputs",
        ),
    )

    # 5. í•™ìŠµ ì‹¤í–‰
    print("ğŸ”¥ Training started...")
    trainer.train()

    # 6. GGUF ë³€í™˜ ë° ì €ì¥ (Ollamaìš©)
    print("ğŸ’¾ Converting to GGUF format...")
    # unslothëŠ” ë‚´ë¶€ì ìœ¼ë¡œ llama.cpp ë³€í™˜ ê¸°ëŠ¥ì„ ì œê³µí•¨
    model.save_pretrained_gguf("model_gguf", tokenizer, quantization_method = "q4_k_m")
    
    # 7. Ollama ëª¨ë¸ ìƒì„±
    print(f"ğŸ³ Creating Ollama model: {new_model_name}...")
    
    modelfile_content = f"""
FROM ./model_gguf/{base_model_name.split('/')[-1]}-Q4_K_M.gguf
TEMPLATE \"\"\"{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
\"\"\"
PARAMETER stop "<|im_start|>"
PARAMETER stop "<|im_end|>"
"""
    with open("Modelfile", "w") as f:
        f.write(modelfile_content)

    try:
        run_command(f"ollama create {new_model_name} -f Modelfile")
        print(f"âœ… Ollama model '{new_model_name}' created successfully!")
    except Exception as e:
        print(f"âš ï¸ Failed to create Ollama model: {e}")
        print("You can manually create it using: ollama create qwen-stock-trader -f Modelfile")

if __name__ == "__main__":
    try:
        # ë² ì´ìŠ¤ ëª¨ë¸ì„ Qwen2.5ë¡œ ë³€ê²½
        train(base_model_name="unsloth/Qwen2.5-7B-Instruct-bnb-4bit")
    except Exception as e:
        print(f"âŒ Training failed: {e}")

"""
Train Local Model - ë¡œì»¬ LLM ë¯¸ì„¸ì¡°ì • (Fine-tuning)
Unslothë¥¼ ì‚¬ìš©í•˜ì—¬ Qwen/Llama ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ê³ , GGUFë¡œ ë³€í™˜í•˜ì—¬ Ollamaì— ë“±ë¡í•©ë‹ˆë‹¤.
"""
import os
import torch
import subprocess
from dataset_builder import DatasetBuilder
from transformers import TrainingArguments
from trl import SFTTrainer

# Unsloth ë¼ì´ë¸ŒëŸ¬ë¦¬ (í•„ìˆ˜)
try:
    from unsloth import FastLanguageModel
    HAS_UNSLOTH = True
except ImportError:
    HAS_UNSLOTH = False
    print("âš ï¸ Unsloth not found. Please install it for efficient training.")

def run_command(cmd):
    """ì‰˜ ëª…ë ¹ì–´ ì‹¤í–‰"""
    print(f"Executing: {cmd}")
    process = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
    out, err = process.communicate()
    if process.returncode != 0:
        print(f"Error: {err.decode('utf-8')}")
        raise Exception(f"Command failed: {cmd}")
    return out.decode('utf-8')

def train_and_register_ollama(base_model_name = "unsloth/Qwen2.5-7B-Instruct-bnb-4bit", 
                              new_model_name = "qwen-stock-trader"):
    print(f"ğŸš€ Starting training pipeline for {base_model_name}...")
    
    if not HAS_UNSLOTH:
        raise ImportError("Unsloth library is required for this pipeline. (pip install unsloth)")

    # 1. ë°ì´í„°ì…‹ ì¤€ë¹„
    builder = DatasetBuilder()
    data_files, processed_ids = builder.get_all_data_files(new_only=True)
    
    if not data_files:
        print("âš ï¸ No data files found. Skipping training.")
        return

    from datasets import load_dataset
    # ì—¬ëŸ¬ íŒŒì¼ì„ í•˜ë‚˜ì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ë¡œë“œ
    dataset = load_dataset("json", data_files=data_files, split="train")
    print(f"ğŸ“š Loaded {len(dataset)} training examples from {len(data_files)} files")

    # 2. ëª¨ë¸ ë¡œë“œ (Qwen 2.5)
    max_seq_length = 2048
    model, tokenizer = FastLanguageModel.from_pretrained(
        model_name = base_model_name,
        max_seq_length = max_seq_length,
        dtype = None,
        load_in_4bit = True,
    )
    
    # LoRA ì–´ëŒ‘í„° ì¶”ê°€
    model = FastLanguageModel.get_peft_model(
        model,
        r = 16,
        target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                          "gate_proj", "up_proj", "down_proj",],
        lora_alpha = 16,
        lora_dropout = 0,
        bias = "none",
        use_gradient_checkpointing = True,
    )

    # 3. í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ… (Qwen ChatML ìŠ¤íƒ€ì¼)
    # Qwenì€ ChatML í¬ë§·ì„ ì‚¬ìš©í•˜ë¯€ë¡œ ì´ì— ë§ì¶°ì•¼ í•¨
    def formatting_prompts_func(examples):
        instructions = examples["instruction"]
        outputs = examples["output"]
        texts = []
        for instruction, output in zip(instructions, outputs):
            # ChatML Format
            text = f"<|im_start|>user\n{instruction}<|im_end|>\n<|im_start|>assistant\n{output}<|im_end|>"
            texts.append(text)
        return {"text": texts}

    # 4. í•™ìŠµ ì„¤ì •
    trainer = SFTTrainer(
        model = model,
        tokenizer = tokenizer,
        train_dataset = dataset,
        dataset_text_field = "text",
        max_seq_length = max_seq_length,
        dataset_num_proc = 2,
        formatting_func = formatting_prompts_func,
        args = TrainingArguments(
            per_device_train_batch_size = 2,
            gradient_accumulation_steps = 4,
            warmup_steps = 5,
            max_steps = 60, # ë°ì´í„° ì–‘ì— ë”°ë¼ ì¡°ì • í•„ìš”
            learning_rate = 2e-4,
            fp16 = not torch.cuda.is_bf16_supported(),
            bf16 = torch.cuda.is_bf16_supported(),
            logging_steps = 1,
            optim = "adamw_8bit",
            weight_decay = 0.01,
            lr_scheduler_type = "linear",
            seed = 3407,
            output_dir = "outputs",
        ),
    )

    # 5. í•™ìŠµ ì‹¤í–‰
    print("ğŸ”¥ Training started...")
    trainer.train()

    # 6. GGUF ë³€í™˜ ë° ì €ì¥ (Ollamaìš©)
    print("ğŸ’¾ Converting to GGUF format...")
    # unslothëŠ” ë‚´ë¶€ì ìœ¼ë¡œ llama.cpp ë³€í™˜ ê¸°ëŠ¥ì„ ì œê³µí•¨
    model.save_pretrained_gguf("model_gguf", tokenizer, quantization_method = "q4_k_m")
    
    # 7. Ollama ëª¨ë¸ ìƒì„± (ë²„ì „ ê´€ë¦¬)
    from datetime import datetime
    version_tag = datetime.now().strftime("%Y%m%d")
    versioned_model_name = f"{new_model_name}:{version_tag}"
    latest_model_name = f"{new_model_name}:latest"
    
    print(f"ğŸ³ Creating Ollama model: {versioned_model_name}...")
    
    modelfile_content = f"""
FROM ./model_gguf/{base_model_name.split('/')[-1]}-Q4_K_M.gguf
TEMPLATE \"\"\"{{ if .System }}<|im_start|>system
{{ .System }}<|im_end|>
{{ end }}{{ if .Prompt }}<|im_start|>user
{{ .Prompt }}<|im_end|>
{{ end }}<|im_start|>assistant
\"\"\"
PARAMETER stop "<|im_start|>"
PARAMETER stop "<|im_end|>"
"""
    with open("Modelfile", "w") as f:
        f.write(modelfile_content)

    try:
        # ë²„ì „ë³„ ëª¨ë¸ ìƒì„±
        run_command(f"ollama create {versioned_model_name} -f Modelfile")
        print(f"âœ… Created versioned model: {versioned_model_name}")
        
        # latest íƒœê·¸ ê°±ì‹  (ë³µì‚¬)
        run_command(f"ollama cp {versioned_model_name} {latest_model_name}")
        print(f"âœ… Updated latest model: {latest_model_name}")
        
        # êµ¬ë²„ì „ ì •ë¦¬ (ìµœì‹  3ê°œë§Œ ìœ ì§€)
        manage_old_models(new_model_name)
        
        # 8. í•™ìŠµ ì™„ë£Œëœ ë°ì´í„° ë§ˆí‚¹ (ì¬í•™ìŠµ ë°©ì§€)
        if processed_ids:
            builder.mark_processed(processed_ids)
            print(f"âœ… Marked {len(processed_ids)} records as trained.")
            
    except Exception as e:
        print(f"âš ï¸ Failed to create Ollama model: {e}")

def manage_old_models(base_name="qwen-stock-trader", keep_count=3):
    """ì˜¤ë˜ëœ Ollama ëª¨ë¸ ë²„ì „ ì‚­ì œ"""
    try:
        # ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
        output = run_command("ollama list")
        lines = output.strip().split('\n')[1:] # í—¤ë” ì œì™¸
        
        # í•´ë‹¹ ë² ì´ìŠ¤ ì´ë¦„ì„ ê°€ì§„ ëª¨ë¸ í•„í„°ë§ (latest ì œì™¸)
        versions = []
        for line in lines:
            parts = line.split()
            if not parts: continue
            name = parts[0]
            if name.startswith(f"{base_name}:") and not name.endswith(":latest"):
                versions.append(name)
        
        # ì´ë¦„ìˆœ ì •ë ¬ (ë‚ ì§œ íƒœê·¸ì´ë¯€ë¡œ ë¬¸ìì—´ ì •ë ¬ = ë‚ ì§œ ì •ë ¬)
        versions.sort(reverse=True) # ìµœì‹ ìˆœ
        
        # keep_count ì´ˆê³¼ë¶„ ì‚­ì œ
        if len(versions) > keep_count:
            to_delete = versions[keep_count:]
            for model in to_delete:
                print(f"ğŸ—‘ï¸ Deleting old model: {model}")
                run_command(f"ollama rm {model}")
                
    except Exception as e:
        print(f"âš ï¸ Failed to cleanup old models: {e}")

if __name__ == "__main__":
    try:
        # ë² ì´ìŠ¤ ëª¨ë¸ì„ Qwen2.5ë¡œ ë³€ê²½
        train_and_register_ollama(base_model_name="unsloth/Qwen2.5-7B-Instruct-bnb-4bit")
    except Exception as e:
        print(f"âŒ Training failed: {e}")
